name: Build llama.cpp with CUDA Backend

on:
  workflow_dispatch:
    inputs:
      version:
        description: 'Commit or tag to checkout'
        default: 'master'
        required: false
        type: string
      cuda_version:
        description: 'CUDA version to use (e.g., 12.4.1)'
        default: '12.4.1'
        required: true
        type: string
      cuda_arch:
        description: 'CUDA arch 70,75 or 80'
        default: '70,75,80,86'
        required: true
        type: string
      os_version:
        description: 'OS version: ubuntu-latest, ubuntu-22.04, etc.'
        default: 'ubuntu-22.04'
        required: true
        type: string

jobs:
  build_llama_cpp:
    name: Build llama.cpp CUDA on ${{ inputs.os_version }}
    runs-on: ${{ inputs.os_version }}
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
        with:
          ref: ${{ inputs.version }}


      - name: Free up disk space
        if: ${{ runner.os == 'Linux' }}
        # https://github.com/easimon/maximize-build-space/blob/master/action.yml
        # https://github.com/easimon/maximize-build-space/tree/test-report
        run: |
          sudo rm -rf /usr/share/dotnet
          sudo rm -rf /opt/ghc
          sudo rm -rf /opt/hostedtoolcache/CodeQL

      - name: Set up CUDA Toolkit
        uses: Jimver/cuda-toolkit@v0.2.26
        with:
          cuda: ${{ inputs.cuda_version }}
          linux-local-args: 
            '["--toolkit"]'
          # non-cuda-sub-packages:
            # '["libcublas", "libcufft"]'
          method: local
          # sub-packages: '["nvcc", "cudart", "thrust", "cublas"]'

      - name: Install dependencies
        run: |
          sudo apt update
          sudo apt install -y build-essential cmake git curl libcurl4-openssl-dev

      - name: Configure and build llama.cpp with CUDA
        run: |
          cd ${{ github.workspace }}
          rm -rf build
          mkdir -p build
          cd build

          cmake .. \
            -DGGML_RPC=ON \
            -DGGML_CUDA=ON \
            -DGGML_CUDA_F16=ON \
            -DGGML_CUDA_FA_ALL_QUANTS=ON \
            -DCMAKE_CUDA_ARCHITECTURES=${{ inputs.cuda_arch }} \
            -DCMAKE_CUDA_COMPILER=$(which nvcc)

          cmake --build . --config Release -j$(nproc)

      - name: List built binaries
        run: |
          ls -lh build/bin/

      - name: Upload artifacts
        uses: actions/upload-artifact@v4
        with:
          name: llama-cpp-cuda-${{ inputs.cuda_version }}-${{ inputs.os_version }}
          path: build/bin/
